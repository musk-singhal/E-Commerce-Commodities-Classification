{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK-3 NLP \n",
    "\n",
    "<br><b>Filename: <font color='red'>visualizing_results.ipynb</font></b> ---> defines the necessary functions in order to obtain useful statistics from the results and visualize them for enhanced comprehension.\n",
    "<hr/>\n",
    "This notebook specifies the following functions: ( the sequence of description is same as the sequence of their definition in the notebook cells below )\n",
    "<ol>\n",
    "    <li><b>get_stats( actual,predicted ): </b> Given the list of actual and predicted labels, the function returns stnadard inferences that can be obtained from these values, such as accuracy and visual representation of True positives and misclassified records through a pie chart </li>\n",
    "    <li><b>get_heatmap( actual,predicted ):</b> Given the actual and predicted labels, the function returns the heatmap as a visual represntation of the percentage distribution of records of each possible label. </li>\n",
    "    <li><b>autolabel(rect):</b>To display the value over the particular bar in barplot</li>\n",
    "    <li><b>get_grouped_graph( actual,predicted ):</b> Given the actual and predicted labels, the function returns a grouped bar graph as a visual representation of number of records from each label which have been correctly predicted and those which have not been.</li>\n",
    "    <li><b>vis_data(data,ax):</b>Given the dataset, this function visualizes the number of records from each category in the form of a barplot.</li>\n",
    "    <li><b>visualize():</b>Driver function for the visualization pipeline.</li>\n",
    "</ol>\n",
    "\n",
    "<img src='images/visualize.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #1: importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #2: defining get_stats( actual, predicted )\n",
    "Function description in the top cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(actual,predicted):\n",
    "    \n",
    "    \n",
    "    print(\"----------------- THE RESULTS OBTAINED ARE AS FOLLOWS ---------------\")\n",
    "    print(\"\")\n",
    "    print(\"Total number of records: \",len(actual))\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    \n",
    "    #----------------------------- RECORDING THE NUMBER OF RECORDS PREDICTED CORRECTLY AND INCORRECTLY\n",
    "    \n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct+=1\n",
    "        else:\n",
    "            wrong+=1\n",
    "    \n",
    "    #----------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    print(\"Number of records predicted correctly = \",correct)\n",
    "    print(\"Number of records misclassified = \",wrong)\n",
    "    \n",
    "    #------------------------------------------------------------------------- COMPUTING ACCURACY\n",
    "    \n",
    "    print(\"The accuracy of the model is = \",correct/(correct+wrong)*100,\"%\")\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    x = precision_score(actual, predicted, average='macro',labels=np.unique(predicted))\n",
    "    #print(set(actual) - set(predicted))\n",
    "    print(\"Precision is = \",x)\n",
    "    \n",
    "    y = recall_score(actual, predicted , average='macro', labels = np.unique(predicted))\n",
    "    print(\"Recall is = \",y)\n",
    "    \n",
    "    #------------------------- PIE CHART VISUALIZATION CODE SEGMENT\n",
    "    \n",
    "    y = np.array([correct,wrong])\n",
    "    textprops = {\"fontsize\":11}\n",
    "    fig1, ax1 = plt.subplots(figsize=(5,5))\n",
    "    \n",
    "    \n",
    "    ax1.pie(y,labels=[\"Correct Predictions\",\"Incorrect Predictions\"],autopct='%1.4f%%',textprops=textprops)\n",
    "    #plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #3: defining get_heatmap( actual, predicted )\n",
    "Function description in the top cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(actual,predicted):\n",
    "    count = Counter(zip(actual,predicted))\n",
    "    #print(count)\n",
    "    \n",
    "    #------------------------------------- DEFINING AND POPULATING DATAFRAME FOR CONSTRUCTING THE HEATMAP\n",
    "    \n",
    "    df = pd.DataFrame(0,columns=np.unique(actual),index=np.unique(predicted))\n",
    "    \n",
    "    for i,j in count.items():\n",
    "        df[i[0]][i[1]] = j\n",
    "    total = []\n",
    "    s=0\n",
    "    for i in range(len(df.columns)):\n",
    "        for j in range(len(df)):\n",
    "            s = s + df.iloc[j,i]\n",
    "        total.append(s)\n",
    "        s=0\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.columns)):\n",
    "            df.iloc[i,j] = (df.iloc[i,j]/total[j])*100\n",
    "            \n",
    "    #--------------------------------------------------------------------------------------\n",
    "    \n",
    "    #--------------------------------------------------- HEATMAP VISUALIZATION CODE SEGMENT\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(45,20))  \n",
    "    df.replace(0,np.nan)\n",
    "    sns.set(font_scale=2)\n",
    "    plt.xlabel(\"Predicted Value ------>\") \n",
    "    plt.ylabel(\"Actual Value ------>\")\n",
    "    res = sns.heatmap(df,linewidths=0.5,annot=True,fmt='.2f',cmap=\"icefire\",annot_kws={\"size\": 20},cbar_kws={'label': 'Percentage(%)'})\n",
    "    res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 22)\n",
    "    res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 22)\n",
    "    ax.set(xlabel=\"Actual Value -------->\", ylabel = \"Predicted Value -------->\")\n",
    "    ax.set_title('Heatmap for predictions ( in %age )')\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #4: defining get_grouped_graph( actual, predicted )\n",
    "Function description in the top cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects,ax): # ------------- FOR DISPLAYING LABELS OVER EACH BAR OF THE GROUPED BAR GRAPH\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "\n",
    "def get_grouped_graph(actual,predicted):\n",
    "    #print(Counter(actual))\n",
    "    #print(len(actual))\n",
    "    cat_correct = []\n",
    "    cat_wrong = []\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    cat = np.unique(actual).tolist()\n",
    "    #print(cat)\n",
    "    \n",
    "    #------------------------------------------------------------------ PREPARING DATA FOR VISUALIZATION\n",
    "    # for each category, keeping record of how many correct and incorrect predictions in a list\n",
    "    \n",
    "    for i in range(len(cat)):\n",
    "        for j in range(len(predicted)):\n",
    "            if actual[j] == cat[i]:\n",
    "                if predicted[j] == actual[j]:\n",
    "                    correct = correct + 1\n",
    "                else:\n",
    "                    wrong = wrong + 1\n",
    "        if(wrong>=correct):\n",
    "            print(\"The category '\",cat[i],\"' has fewer True Positives (\",correct,\") than misclassifications (\",wrong,\")\")\n",
    "        cat_correct.append(correct)\n",
    "        cat_wrong.append(wrong)\n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    #print(\"Predicted correct = \",cat_correct)\n",
    "    #print(\"Predicted wrong = \",cat_wrong)\n",
    "    \n",
    "    #------------------------------------------------------------ GROUPED BAR GRAPH VISUALIZATION CODE SEGMENT\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(25)\n",
    "    fig.set_figwidth(85)\n",
    "    width = 0.35\n",
    "    plt.rcParams.update({'font.size': 35})\n",
    "    x = np.arange(len(cat))\n",
    "    #x=3*x\n",
    "    rects1 = ax.bar(x - width/2, cat_correct, width, label='Predicted Correctly')\n",
    "    rects2 = ax.bar(x + width/2, cat_wrong, width, label='Predicted Incorrectly',color='red')\n",
    "    \n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Count ------>',fontsize=70)\n",
    "    ax.set_xlabel('Categories ------>',fontsize=70)\n",
    "    ax.set_title('Category wise predicted counts')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cat,rotation=90,fontsize=45)\n",
    "    ax.legend(prop={'size': 70})\n",
    "    \n",
    "    autolabel(rects1,ax)\n",
    "    autolabel(rects2,ax)\n",
    "\n",
    "    #fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #4a): defining vis_data( data )\n",
    "<br> visualizes the dataset after the preprocessing gets completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_data(data):\n",
    "    labels, values = zip(*Counter(data['cat'].tolist()).items())\n",
    "    df = pd.DataFrame(columns=['labels','count'])\n",
    "    df['labels'] = labels\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'count'] = values[i]\n",
    "    df = df.sort_values('count')\n",
    "    fig, ax = plt.subplots(figsize=(45,20))\n",
    "    ax = sns.barplot(x=\"labels\", y=\"count\", data=df, order=df['labels'])\n",
    "    #autolabel(b,ax)\n",
    "    plt.xticks(rotation='vertical',fontsize = 50)\n",
    "    plt.yticks(fontsize = 50)\n",
    "    ax.grid(b=True, which='major', color='#d3d3d3', linewidth=1.0)\n",
    "    ax.grid(b=True, which='minor', color='#d3d3d3', linewidth=0.5)\n",
    "    '''indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "    plt.rcParams.update({'font.size': 35})\n",
    "    b = ax.bar(indexes, values, width)\n",
    "    ax.set_ylabel('Count ------>',fontsize=70)\n",
    "    ax.set_xlabel('Categories ------>',fontsize=70)\n",
    "    autolabel(b,ax)\n",
    "    plt.xticks(indexes + width * 0.05, labels,rotation=90,fontsize=40)'''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #5: defining visualize()\n",
    "<br>Driver function for the implementation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    \n",
    "    print(\"====================================================================\")\n",
    "    print(\"==================== VISUALIZING THE RESULTS =======================\")\n",
    "    print(\"====================================================================\")\n",
    "    filename=[\"output_files/string_svm_results.csv\",\"output_files/bow_results.csv\"]\n",
    "    models=[\"SVM MODEL\",\"BAG OF WORDS MODEL\"]\n",
    "    \n",
    "    for i in range(len(filename)):\n",
    "        f = filename[i]\n",
    "        print(\"\")\n",
    "        print(\"****************************************************************\")\n",
    "        print(\"\")\n",
    "        print(\"                          \",models[i],\"                       \")\n",
    "        print(\"\")\n",
    "        print(\"****************************************************************\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        d = pd.read_csv(f)\n",
    "        #print(np.unique(bow['actual']))\n",
    "        get_stats(d['actual'].tolist() , d['predicted'].tolist())\n",
    "        get_heatmap( d['actual'].tolist() , d['predicted'].tolist() )\n",
    "        get_grouped_graph(d['actual'].tolist() , d['predicted'].tolist())\n",
    "        print(\"\")\n",
    "        print(\"****************************************************************\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
