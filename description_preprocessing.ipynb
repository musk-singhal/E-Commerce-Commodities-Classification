{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>description_preprocessing.ipynb</font>\n",
    "\n",
    "<br><b>Filename: description_preprocessing.ipynb</b> ---> <font color='purple'>defines the implementation pipeline for preprocessing the description for each record in the provided dataset. The adopted preprocessing sub-approaches are: stop-word removal, lemmatization (text normalization), custom word set removal etc.</font>\n",
    "<hr/>\n",
    "This notebook specifies the following functions: ( the sequence of description is same as the sequence of their definition in the notebook cells below )\n",
    "<ol>\n",
    "    <li><b>remove_stopwords(text): </b> Given input text, remove all the stopwords. </li>\n",
    "    <li><b>perform_lemmatization(text):</b> Given the input text, perform text normalization (lemmatization)</li>\n",
    "    <li><b>custom_word_removal( text, brand ):</b> Given the input text, remove a set of custom words ( manually created frequently occurring words in this dataset ) and the brand name from the description.</li>\n",
    "    <li><b>preprocess_description( df ):</b>The driver function for description text preprocessing pipeline.</li>\n",
    "</ol>\n",
    "\n",
    "<img src='images/text_preprocess.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #1: importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #2: defining remove_stopwords(text)\n",
    "<br>Function description in the top cell\n",
    "<br>This function removes the stop words from the input text and returns the updated state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=\"\"\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if not word in stop_words:\n",
    "            new_text+=word+\" \"\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #3: defining perform_lemmatization(text)\n",
    "<br>Function description in the top cell\n",
    "<br>This function normalizes the input text ( lemmatization ) and returns the updated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lemmatization(text):\n",
    "    new_text=\"\"\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        #print(\"Word before: \",word)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        #print(\"Word after: \",word)\n",
    "        new_text+=word+\" \"\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #4: defining custom_word_removal( text,brand )\n",
    "<br>Function description in the top cell\n",
    "<br>This function does the following sequence of operations:\n",
    "<ol>\n",
    "    <li>Define a set of surplus words including the brand name.</li>\n",
    "    <li>Remove all non-alphanumeric characters</li>\n",
    "    <li>Remove all words with length <=2 </li>\n",
    "    <li>Remove all purely numeric strings</li>\n",
    "    <li>Remove duplicate words</li>\n",
    "    <li>Return the updated description text.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_word_removal(text,brand):\n",
    "    new_text=\"\"\n",
    "    \n",
    "    #----------------------------------- STEP-1\n",
    "    \n",
    "    to_remove=[brand,'free','cost','delivery','india','offer','customer','price','sale','warranty','guarantee','satisfaction','replacement','shipping','cash','code','buy','flipkartcom','now','hurry','genuine','product','day','key','specification']\n",
    "    \n",
    "    #----------------------------------- STEP-2\n",
    "    \n",
    "    text = re.sub(r'\\w*[0-9]\\w*',\"\",text)\n",
    "    text = text.split()\n",
    "    \n",
    "    #----------------------------------- STEPS 3,4,AND 5 START HERE\n",
    "    for i in range(len(text)):\n",
    "        if text[i] in to_remove or len(text[i])<=2 or text[i].isnumeric() or text[i] in new_text.split():\n",
    "            continue\n",
    "        else:\n",
    "            new_text+=text[i]+\" \"\n",
    "    #----------------------------------- STEPS 3,4,AND 5 START HERE\n",
    "    \n",
    "    return new_text #----------------------------------- STEP-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL #5: defining preprocess_description( df )\n",
    "<br>The driver function for description preprocessing\n",
    "<br>Function description in the top cell\n",
    "<br>This function does the following sequence of operations:\n",
    "<ol>\n",
    "    <li>Replace multiple spaces with single whitespace.</li>\n",
    "    <li>Convert entire text to lowercase.</li>\n",
    "    <li>Use remove_stopwords() to remove the stop words form the description</li>\n",
    "    <li>Use perform_lemmatization() to normalize the description.</li>\n",
    "    <li>Remove the custom words from description using custom_word_removal()</li>\n",
    "    <li>Record each intermediate stage of function call preprocessing as a separate column in the original dataset.</li>\n",
    "    <li>Remove all records with empty description field.</li>\n",
    "    <li>Return the updated dataset</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_description(df):\n",
    "    print(\"---------------------- PREPROCESSING STARTS HERE\")\n",
    "    ws_description=[]\n",
    "    lem_ws_description=[]\n",
    "    custom = []\n",
    "    for i in range(len(df)):\n",
    "        #print(\"BEFORE:  \",df.loc[i,'description'])\n",
    "        #print(type(df.loc[i,'brand']))\n",
    "        #print(df.loc[i,'brand'])\n",
    "        df.loc[i,'description'] = re.sub('\\s+',' ',df.loc[i,'description']) #------------- STEP-1\n",
    "        \n",
    "        df.loc[i,'description'] = (df.loc[i,'description']).lower() # -------------------- STEP-2\n",
    "        \n",
    "        if type(df.loc[i,'brand'])==str:\n",
    "            df.loc[i,'brand'] = (df.loc[i,'brand']).lower() #---------------- STEP-2 FOR BRAND NAME\n",
    "            \n",
    "        else:\n",
    "            df.loc[i,'brand'] = 'brand' #if any ambiguity in the brand string,replace brand name with 'brand'\\\n",
    "            \n",
    "        df.loc[i,'description'] = \"\".join(e for e in df.loc[i,'description'] if e.isalnum() or e==' ')\n",
    "        df.loc[i,'description'] = (df.loc[i,'description']).replace(df.loc[i,'brand']+\" \",\"\")\n",
    "        \n",
    "        ws_description.append(remove_stopwords(df.loc[i,'description'])) #---------- STEP-3\n",
    "        \n",
    "        lem_ws_description.append(perform_lemmatization(ws_description[i])) #------- STEP-4\n",
    "        \n",
    "        custom.append(custom_word_removal(lem_ws_description[i],df.loc[i,'brand'])) #---------STEP-5\n",
    "        \n",
    "        #print(\"AFTER:  \",df.loc[i,'description'])\n",
    "        #print(\"---------------------------------\")\n",
    "        \n",
    "    print(\"------ PREPROCESSING DONE....\")\n",
    "    \n",
    "    #----------------------------------------------------------------------------- STEP-6 STARTS HERE\n",
    "    \n",
    "    df['ws_description'] = ws_description\n",
    "    df['lem_ws_description'] = lem_ws_description\n",
    "    df['custom'] = custom\n",
    "    \n",
    "    #----------------------------------------------------------------------------- STEP-6 ENDS HERE \n",
    "    \n",
    "    indices=[]\n",
    "    \n",
    "    #----------------------------------------------------------------------------- STEP-7 STARTS HERE\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i,'custom']==\" \" or len(df.loc[i,'custom'])==0 or not df.loc[i,'custom']:\n",
    "            indices.append(i)\n",
    "    df = df.drop(index=indices)\n",
    "    df = df.reset_index(drop=True)\n",
    "    #----------------------------------------------------------------------------- STEP-7 ENDS HERE\n",
    "    \n",
    "    return df #------------------------------------------------------------------ STEP-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
